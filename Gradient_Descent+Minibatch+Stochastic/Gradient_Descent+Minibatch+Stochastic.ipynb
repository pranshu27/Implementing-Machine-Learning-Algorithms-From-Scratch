{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d8523b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63467f58",
   "metadata": {},
   "source": [
    "# Q1.a)  Use this function to find minima for (i) x2 + 3x+4 and (ii) x4 – 3x2 +2x. [5 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38eb88",
   "metadata": {},
   "source": [
    "### 1. $x^2+3x+4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a94b4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-15,15,300)\n",
    "y=x**2+3*x+4\n",
    "plt.xlim([-15,15])\n",
    "plt.ylim([0,10])\n",
    "plt.grid()\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87891b3",
   "metadata": {},
   "source": [
    "## 2. $x^4 - 3x^2 + 2x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bbfee287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x=np.linspace(-15,15,300)\n",
    "y=x**4-3*x*x+2*x\n",
    "plt.xlim([-15,15])\n",
    "plt.ylim([-10,15])\n",
    "plt.grid()\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2232a4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 64\n",
      "Minima for the function x^2+3x+4 is found at -1.5 and the minimum value of the function is 1.75\n",
      "Iterations: 66\n",
      "Minima for the function x^4-3x^2+2x is found at -1.366 and the minimum value of the function is -4.848076206064\n"
     ]
    }
   ],
   "source": [
    "gradient1 = lambda x: 2*x+3 ## specifies gradient for the function x2 + 3x+4\n",
    "gradient2 = lambda x: 4*(x**3)-6*x+2 ## specifies gradient for the function  x4 – 3x2 +2x.\n",
    "\n",
    "def gradient_descent (gradient, init_, learn_rate, n_iter=1000, tol=1e-06):\n",
    "    x = init_\n",
    "    for i in range (n_iter): \n",
    "        delta = (-1)*learn_rate * gradient(x)\n",
    "        if np.all(np.abs (delta) <= tol): \n",
    "            break \n",
    "        x += delta \n",
    "    print('Iterations:', str(i+1))\n",
    "    return  round(x*1000)/1000\n",
    "\n",
    "x1 = gradient_descent(gradient1,4 , 0.1)\n",
    "min_func1 = x1**2+3*x1+4\n",
    "print('Minima for the function x^2+3x+4 is found at '+ str(x1), 'and the minimum value of the function is', str(min_func1))\n",
    "\n",
    "x2 = gradient_descent(gradient2,-1 , 0.01)\n",
    "min_func2 = x2**4 - 3*(x2**2) + 2*x2\n",
    "print('Minima for the function x^4-3x^2+2x is found at '+ str(x2), 'and the minimum value of the function is', str(min_func2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a2d8a1",
   "metadata": {},
   "source": [
    "## NOTE: The first polynomial is a convex function, so initialisation won't matter a lot. But the second polynomial is a non convex function, and hence the initialization value matters. I tried initialising with the value **~4**, and ended up finding local minima. Finally, the initial value of **~-1** is giving me the global minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91d3e9b",
   "metadata": {},
   "source": [
    "# Q1.b) Write a gradient function to calculate gradients for a linear regression y = ax + b [10 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74212e1",
   "metadata": {},
   "source": [
    "The gradients w.r.t __a__ and __b__ are calculated as: \n",
    "$$\\nabla_a = \\frac{-2}{n} * \\sum(X * (Y - Y_{pred}))$$\n",
    "$$\\nabla_b = \\frac{-2}{n} * \\sum(Y - Y_pred)$$\n",
    "<br>\n",
    "\n",
    "$where, \\; Y_{pred} = a\\ast X + b$,<br> n = number of points in the dataset, <br>X is the input vector, <br>Y is the output vector \n",
    "\n",
    "### The same are used in the function below to elaborate their actual usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46c907fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(X, Y):\n",
    "    Y_pred = a*X + b \n",
    "        \n",
    "    D_a = (-2/n) * np.dot(X.T,(Y - Y_pred)) # gradient wrt a \n",
    "    D_b = (-2/n) * sum(Y - Y_pred) # gradient wrt b\n",
    "    return D_a, D_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776bf08e",
   "metadata": {},
   "source": [
    "# Q1.c) Generate artificial data for this regression according to the following protocol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "025d9c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the parameters to be used later\n",
    "my_iter = 1000\n",
    "my_tol = 1e-06\n",
    "my_lr = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717fbb2",
   "metadata": {},
   "source": [
    "### Here we are trying to generate some artificial data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a92cea38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating some random data \n",
    "\n",
    "np.random.seed (0)\n",
    "X = 2.5*np.random.randn(10000)+1.5 #Ar\n",
    "res=1.5*np.random.randn(10000) # Gene.\n",
    "Y = 2+0.3*X + res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded139b9",
   "metadata": {},
   "source": [
    "##  Here we are trying to calculate partial derivatives in each iteration, w.r.t to the free parameteres __a__ and __b__. We updated their values based on the learning rate and the gradients calculated, and break the loop under following two scenarios:\n",
    "### 1. Iterations have equalled to max_iter\n",
    "### 2. Delta_a and delta_b have dropped below a specified tolerance, $10^{-6}$, in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5900b39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are  (0.3, 2.02, 0.4080841541290283, 1000)\n"
     ]
    }
   ],
   "source": [
    "batch_time = 0\n",
    "def lr_gd(X, Y, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    start_time = time.time()\n",
    "\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = X.shape[0] \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        Y_pred = a*X + b \n",
    "        \n",
    "        D_a = (-2/n) * np.dot(X.T,(Y - Y_pred))  \n",
    "        D_b = (-2/n) * sum(Y - Y_pred)  \n",
    "        \n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "        \n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #batch_time = time.time() - start_time\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return round(a,2), round(b,2),time.time() - start_time, i+1 \n",
    "\n",
    "a,b,batch_time, iterations = lr_gd(X, Y)\n",
    "print('\\nUsing batch gradient descent with a learning rate of 0.01, the values of (a,b, execution time, #iterations) are ', (a,b,batch_time, iterations))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d353060",
   "metadata": {},
   "source": [
    "# Q1.d) Implement minibatch stochastic gradient descent using the code base you have developed so far.[15 points]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53555593",
   "metadata": {},
   "source": [
    "## Stochastic- Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "51b202b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_stochastic_gd(X, Y, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = X.shape[0] \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        #np.random.seed (5)\n",
    "        random_index=np.random.randint(0,n-1)\n",
    "        X_mini= X[random_index]\n",
    "        Y_mini = Y[random_index]\n",
    "        \n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "       # print(X_mini, Y_mini)\n",
    "\n",
    "\n",
    "        D_a = (-2)*X_mini*(Y_mini - Y_pred)  \n",
    "        D_b = (-2)*(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c74594",
   "metadata": {},
   "source": [
    "## Since there is a large amount of fluctuation in SGD, we will try to run the SGD method multiple times, and stop when we get the values of our free parameters, __a__ and __b__ in our desired range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315f0a9f",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ca0f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.0076160430908203125 0.34 2.0\n",
      "1000 0.007387876510620117 0.36 1.99\n",
      "1000 0.007524967193603516 0.47 2.18\n",
      "1000 0.007325887680053711 0.1 2.01\n",
      "1000 0.007283210754394531 0.26 2.06\n",
      "1000 0.007048130035400391 0.42 2.06\n",
      "1000 0.006937742233276367 0.23 1.93\n",
      "685 0.0047528743743896484 0.35 1.88\n",
      "1000 0.007102012634277344 0.37 1.97\n",
      "1000 0.00737309455871582 0.24 1.95\n",
      "1000 0.0073549747467041016 0.11 1.96\n",
      "1000 0.007309913635253906 0.36 2.11\n",
      "1000 0.00745081901550293 0.21 2.0\n",
      "1000 0.007350921630859375 0.29 2.02\n",
      "1000 0.007172107696533203 0.21 2.27\n",
      "1000 0.006978034973144531 0.15 1.99\n",
      "1000 0.007027149200439453 0.31 2.03\n",
      "1000 0.007027864456176758 0.26 1.98\n",
      "1000 0.0069501399993896484 0.24 2.11\n",
      "928 0.006754159927368164 0.43 2.07\n",
      "1000 0.007205963134765625 0.38 2.35\n",
      "1000 0.007725715637207031 0.33 2.13\n",
      "1000 0.0072231292724609375 0.45 1.97\n",
      "1000 0.007185935974121094 0.25 2.12\n",
      "1000 0.007153034210205078 0.27 2.08\n",
      "1000 0.00720524787902832 0.39 1.98\n",
      "1000 0.0070819854736328125 0.4 1.97\n",
      "1000 0.007089853286743164 0.38 2.05\n",
      "1000 0.007144927978515625 0.26 1.92\n",
      "1000 0.007174968719482422 0.22 2.12\n",
      "1000 0.006951808929443359 0.36 2.04\n",
      "1000 0.0071620941162109375 0.23 1.92\n",
      "1000 0.007696866989135742 0.22 2.01\n",
      "1000 0.007700920104980469 0.23 2.17\n",
      "1000 0.007444143295288086 0.47 1.97\n",
      "1000 0.007737874984741211 0.36 2.06\n",
      "1000 0.007226705551147461 0.53 1.94\n",
      "1000 0.007776975631713867 0.35 2.33\n",
      "1000 0.0072400569915771484 0.19 2.0\n",
      "1000 0.0077130794525146484 0.43 1.93\n",
      "1000 0.007279872894287109 0.49 2.15\n",
      "1000 0.007776975631713867 0.24 2.07\n",
      "1000 0.008320093154907227 0.24 2.08\n",
      "1000 0.008037805557250977 0.51 2.12\n",
      "1000 0.00771021842956543 0.3 2.11\n",
      "1000 0.008178949356079102 0.21 2.18\n",
      "1000 0.00757908821105957 0.26 2.03\n",
      "1000 0.008047819137573242 0.28 1.89\n",
      "1000 0.007874250411987305 0.45 1.99\n",
      "1000 0.008601188659667969 0.36 1.95\n",
      "1000 0.00767207145690918 0.23 2.15\n",
      "1000 0.008416175842285156 0.13 2.17\n",
      "783 0.005647897720336914 0.33 2.02\n",
      "1000 0.008532047271728516 0.36 1.95\n",
      "1000 0.00766301155090332 0.42 2.04\n",
      "1000 0.007947921752929688 0.23 2.1\n",
      "1000 0.007793903350830078 0.39 2.03\n",
      "1000 0.007810115814208984 0.39 2.11\n",
      "1000 0.008317947387695312 0.4 1.88\n",
      "1000 0.008533000946044922 0.31 2.19\n",
      "1000 0.008018970489501953 0.38 1.93\n",
      "1000 0.00821685791015625 0.33 2.03\n",
      "1000 0.0077860355377197266 0.35 2.02\n",
      "1000 0.007875919342041016 0.37 2.13\n",
      "1000 0.007319927215576172 0.22 2.01\n",
      "1000 0.007627010345458984 0.19 2.07\n",
      "402 0.0028748512268066406 0.38 2.02\n",
      "1000 0.007277011871337891 0.26 1.9\n",
      "1000 0.007523059844970703 0.39 1.97\n",
      "1000 0.007348060607910156 0.38 1.93\n",
      "1000 0.007298946380615234 0.03 2.12\n",
      "1000 0.00732421875 0.33 2.01\n",
      "1000 0.0076160430908203125 0.16 2.05\n",
      "1000 0.0071370601654052734 0.15 2.1\n",
      "1000 0.007628917694091797 0.4 1.96\n",
      "1000 0.007679939270019531 0.31 2.05\n",
      "1000 0.007691860198974609 0.2 2.0\n",
      "1000 0.007580995559692383 0.32 2.08\n",
      "1000 0.007340192794799805 0.33 2.17\n",
      "1000 0.007427215576171875 0.23 2.28\n",
      "1000 0.007337093353271484 0.3 2.01\n",
      "1000 0.007337093353271484\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    it, exec_time, a,b = lr_stochastic_gd(X, Y)\n",
    "    print(it, exec_time, a, b)\n",
    "    if((a in [0.29, 0.3, 0.31]) and (b in [1.99, 2.0, 2.01])):\n",
    "        break\n",
    "\n",
    "print(it, exec_time)\n",
    "sgd_time = exec_time\n",
    "sgd_iterations = it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9380266",
   "metadata": {},
   "source": [
    "## Here we are creating a function for calculating the LR parameters using Minibatch GD\n",
    "### Steps:\n",
    "1. For each iteration, we are sampling the data from our input dataset of size equal to the batch size provided\n",
    "2. We will calculate the partial derivatives wrt __a__ and __b__\n",
    "3. We will update the values of __a__ and __b__ and calulate the delta of these params\n",
    "4. We will check if the delta values so calcuted fall below the pre-decided tolerance level\n",
    "5. If 4 happens to be true, we will stop and report the values of __a__ and __b__ \n",
    "6. If 4 is false, we will iterate till max_iter is reached, and return the values of __a__ and __b__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e2fb424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_mb_gd(X, Y, batch_size, learning_rate = my_lr, init_a = 0, init_b = 0, max_iter = my_iter, tol = my_tol):\n",
    "    start_time = time.time()\n",
    "    a = 0\n",
    "    b = 0\n",
    "\n",
    "    n = float(len(X)) \n",
    "    \n",
    "    for i in range(max_iter): \n",
    "        #np.random.seed (5)\n",
    "        random_index=np.random.randint(0,n-batch_size-1)\n",
    "        X_mini= X[random_index:random_index+batch_size]\n",
    "        Y_mini = Y[random_index:random_index+batch_size]\n",
    "        \n",
    "        n_tmp = len(X_mini)\n",
    "        Y_pred = a*X_mini + b \n",
    "\n",
    "        D_a = (-2/n_tmp) * np.dot(X_mini.T,(Y_mini - Y_pred))  \n",
    "        D_b = (-2/n_tmp) * sum(Y_mini - Y_pred)  \n",
    "\n",
    "        delta_a = (-1)*learning_rate * D_a\n",
    "        delta_b = (-1)*learning_rate * D_b\n",
    "\n",
    "        if (np.all(np.abs (delta_a) <= tol) & np.all(np.abs (delta_b) <= tol)): \n",
    "            break\n",
    "\n",
    "        a+=delta_a\n",
    "        b+=delta_b\n",
    "\n",
    "# =============================================================================\n",
    "#         print(i)\n",
    "# =============================================================================\n",
    "    #print('It took ', str(i+1), ' iterations to arrive at the desired result')\n",
    "    #print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    #print (round(a,2), round(b,2))\n",
    "    return str(i+1), time.time() - start_time, round(a,2), round(b,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7c3b4",
   "metadata": {},
   "source": [
    "### Now we will calculate the number of iterations, execution time for batch sizes in the range, \n",
    "## $$2^1, 2^2\\; ..... \\; 2^{10}$$\n",
    "\n",
    "### and try to find the values which give the most optimum values of __a__ and __b__ \n",
    "\n",
    "### NOTE: We are iterating over powers of 2 because it is __computationally faster__. Also powers of 2 fit the memory requirements of CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c65aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 2, 'iterations': '1000', 'exec_time': 0.013514041900634766, 'a': 0.36, 'b': 2.13}\n",
      "{'batch_size': 4, 'iterations': '1000', 'exec_time': 0.013070106506347656, 'a': 0.28, 'b': 1.98}\n",
      "{'batch_size': 8, 'iterations': '1000', 'exec_time': 0.011822938919067383, 'a': 0.32, 'b': 1.99}\n",
      "{'batch_size': 16, 'iterations': '1000', 'exec_time': 0.01166534423828125, 'a': 0.28, 'b': 2.01}\n",
      "{'batch_size': 32, 'iterations': '1000', 'exec_time': 0.012679100036621094, 'a': 0.24, 'b': 2.01}\n",
      "{'batch_size': 64, 'iterations': '1000', 'exec_time': 0.014554977416992188, 'a': 0.29, 'b': 2.02}\n",
      "{'batch_size': 128, 'iterations': '1000', 'exec_time': 0.017699003219604492, 'a': 0.3, 'b': 2.0}\n",
      "{'batch_size': 256, 'iterations': '1000', 'exec_time': 0.020943880081176758, 'a': 0.29, 'b': 2.03}\n",
      "{'batch_size': 512, 'iterations': '1000', 'exec_time': 0.03059220314025879, 'a': 0.29, 'b': 2.02}\n",
      "{'batch_size': 1024, 'iterations': '1000', 'exec_time': 0.052922964096069336, 'a': 0.29, 'b': 2.01}\n",
      "   batch_size iterations  exec_time     a     b\n",
      "0           2       1000   0.013514  0.36  2.13\n",
      "1           4       1000   0.013070  0.28  1.98\n",
      "2           8       1000   0.011823  0.32  1.99\n",
      "3          16       1000   0.011665  0.28  2.01\n",
      "4          32       1000   0.012679  0.24  2.01\n",
      "5          64       1000   0.014555  0.29  2.02\n",
      "6         128       1000   0.017699  0.30  2.00\n",
      "7         256       1000   0.020944  0.29  2.03\n",
      "8         512       1000   0.030592  0.29  2.02\n",
      "9        1024       1000   0.052923  0.29  2.01\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [2**i for i in range(1,11)]\n",
    "out = []\n",
    "\n",
    "for b in batch_sizes:\n",
    "    #print('batch_size ', str(b))\n",
    "    tmp = {}\n",
    "    tmp['batch_size'] = b\n",
    "    tmp['iterations'], tmp['exec_time'], tmp['a'], tmp['b'] = lr_mb_gd(X, Y, b)\n",
    "    #print('\\n')\n",
    "    print(tmp)\n",
    "    out.append(tmp)\n",
    "df = pd.DataFrame(out)\n",
    "print(df.head(50))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef8320a",
   "metadata": {},
   "source": [
    "## As observed, we are getting the most optimal values for a and b using a batch size and time taken(seconds) as under:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cce392fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017699003219604492 128 1000\n"
     ]
    }
   ],
   "source": [
    "mgd_time = df[(df['a'].isin([0.30,0.31,0.29])) & (df['b'].isin([1.99, 2.0, 2.01]))]['exec_time'].min()\n",
    "mgd_batch_size = list(df[df['exec_time'] == mgd_time]['batch_size'])[0]\n",
    "mgd_iterations = list(df[df['exec_time'] == mgd_time]['iterations'])[0]\n",
    "\n",
    "print(mgd_time,mgd_batch_size,mgd_iterations)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e5ecb",
   "metadata": {},
   "source": [
    "# Q1.e) Does SGD do better or worse in terms of time performance on our data? Is there an optimal minibatch size that works best? Quantify and interpret your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92037",
   "metadata": {},
   "source": [
    "# Inference: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9a28c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4080841541290283 0.017699003219604492 0.007337093353271484\n"
     ]
    }
   ],
   "source": [
    "print(batch_time, mgd_time, sgd_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507d712",
   "metadata": {},
   "source": [
    "## With fixed values for parameters like **learn rate = 0.005, and max_iter = 1000**, we can observe the time taken by the above three algorithms as:\n",
    "#### $$Batch\\; Gradient\\; Descent > MiniBatch\\; Gradient\\; Descent > Stochastic\\; Gradient\\; Descent$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b363ab50",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5aa1fec",
   "metadata": {},
   "source": [
    "### The percentage reduction in execution time of Minibatch GD versus Batch GD is as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e144e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a64ed629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MGD Time_reduction_perc versus Batch GD = 95.66 %\n"
     ]
    }
   ],
   "source": [
    "print('MGD Time_reduction_perc versus Batch GD =',round(((batch_time - mgd_time)/batch_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635f759",
   "metadata": {},
   "source": [
    "### The percentage reduction in execution time of Stochastic GD versus Batch GD is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe1c16bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Time_reduction_perc versus Batch GD = 98.2 %\n"
     ]
    }
   ],
   "source": [
    "print('SGD Time_reduction_perc versus Batch GD =',round(((batch_time - sgd_time)/batch_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b72f7c",
   "metadata": {},
   "source": [
    "### The percentage reduction in execution time of Stochastic GD versus Minibatch GD is as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3ae217ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Time_reduction_perc versus MGD = 58.55 %\n"
     ]
    }
   ],
   "source": [
    "print('SGD Time_reduction_perc versus MGD =',round(((mgd_time - sgd_time)/mgd_time)*100,2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b3741",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## The most optimal batch size reported so for for Minibatch Grdadient Descent is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12232941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(mgd_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5119cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2920f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4976e0179d97dd6d59b1329a76e601e17b789c2571b41c8b57f5fd69821c0dd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
